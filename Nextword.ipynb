{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csvel\\AppData\\Local\\Temp\\ipykernel_20008\\3299783850.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pkg_resources\n",
    "import pickle\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file \n",
    "\n",
    "def load_doc(filename):\n",
    "    file= open(filename,'r')\n",
    "\n",
    "    text= file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc=doc.replace('--',' ')\n",
    "    tokens=doc.split()\n",
    "    table=str.maketrans('','', string.punctuation)\n",
    "    tokens= [w.translate(table) for w in tokens]\n",
    "\n",
    "    tokens =[word for word in tokens if word.isalpha()]\n",
    "\n",
    "    tokens =[word.lower() for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def save_doc(lines,filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    print(\"Starting Cleaning Process\")\n",
    "    \n",
    "    tokenized_sentences =[]\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "\n",
    "        sentence= cleanhtml(sentence)\n",
    "        sentence= replace_urls(sentence)\n",
    "        sentence= remove_email(sentence)\n",
    "        sentence= re.sub(r'[^a-zA-Z]',' ',sentence)\n",
    "        sentence= sentence.lower()\n",
    "        sentence= misc(sentence)\n",
    "\n",
    "        tokenized_sentences.append(sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    cleantext= re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def replace_urls(data):\n",
    "    url_pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data= url_pattern.sub(r'', data)\n",
    "    return data\n",
    "\n",
    "def remove_email(data):\n",
    "    data=re.sub('\\S*@\\S*\\s?', '', data)\n",
    "    return data\n",
    "\n",
    "def misc(data):\n",
    "    data=re.sub('\\s+',' ', data)\n",
    "\n",
    "    data=re.sub(\"\\'\",\"\", data)\n",
    "    data=re.sub(\"ww+\",\"\",data)\n",
    "\n",
    "    roman= re.compile(r'(\\b[MDCLXVI]+\\b)(\\.)?', re.I)\n",
    "    data=re.sub(roman,\"\", data)\n",
    "    return data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def littleCleaning(sentences):\n",
    "    print(\"Starting cleaning Process\")\n",
    "    ret_list=[]\n",
    "    for sentence in sentences:\n",
    "        words= sentence.split(\" \")\n",
    "        if len(words)>5:\n",
    "            ret_list.append(sentence)\n",
    "        else:\n",
    "            continue\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus is:  1213703\n"
     ]
    }
   ],
   "source": [
    "path=('republic.txt')\n",
    "text= open(path).read().lower()\n",
    "\n",
    "print('length of the corpus is: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Normalization Process\n",
      "Starting Cleaning Process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3395cdec134c448f6d3ae36a99dbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning Process\n",
      "Normalization process finished\n"
     ]
    }
   ],
   "source": [
    "data_list=text.split(\".\")\n",
    "norm_sentence=[]\n",
    "\n",
    "def norm_pipeline(sentences):\n",
    "    print(\"Starting Normalization Process\")\n",
    "    sentences= tokenize(sentences)\n",
    "    sentences=littleCleaning(sentences)\n",
    "    print(\"Normalization process finished\")\n",
    "    return sentences\n",
    "\n",
    "pro_sentences= norm_pipeline(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6498"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pro_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataText=\"\".join(pro_sentences[:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 21747\n",
      "Unique Tokens:  3545\n"
     ]
    }
   ],
   "source": [
    "tokens=clean_doc(dataText)\n",
    "print('Total tokens:', len(tokens))\n",
    "print('Unique Tokens: ', len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences:  21696\n"
     ]
    }
   ],
   "source": [
    "length=50+1\n",
    "\n",
    "Sequences= list()\n",
    "for i in range(length,len(tokens)):\n",
    "    seq= tokens[i-length:i]\n",
    "\n",
    "    line=' '. join(seq)\n",
    "\n",
    "    Sequences.append(line)\n",
    "\n",
    "print('Total Sequences: ', len(Sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_doc(Sequences,filename='republic_sequences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=load_doc('republic_sequences.txt')\n",
    "lines=doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences= tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=array(sequences)\n",
    "\n",
    "X,y =sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "y=to_categorical(y,num_classes=vocab_size)\n",
    "seq_length=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            177300    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 50)            20200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3546)              180846    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 401,096\n",
      "Trainable params: 401,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "170/170 [==============================] - 31s 28ms/step - loss: 6.6366 - accuracy: 0.0834\n",
      "Epoch 2/50\n",
      "170/170 [==============================] - 5s 27ms/step - loss: 6.1277 - accuracy: 0.0844\n",
      "Epoch 3/50\n",
      "170/170 [==============================] - 4s 24ms/step - loss: 5.9872 - accuracy: 0.0906\n",
      "Epoch 4/50\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 5.8440 - accuracy: 0.1152\n",
      "Epoch 5/50\n",
      "170/170 [==============================] - 4s 24ms/step - loss: 5.7093 - accuracy: 0.1213\n",
      "Epoch 6/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 5.6061 - accuracy: 0.1291\n",
      "Epoch 7/50\n",
      "170/170 [==============================] - 5s 28ms/step - loss: 5.5162 - accuracy: 0.1332\n",
      "Epoch 8/50\n",
      "170/170 [==============================] - 5s 31ms/step - loss: 5.4407 - accuracy: 0.1350\n",
      "Epoch 9/50\n",
      "170/170 [==============================] - 4s 25ms/step - loss: 5.3714 - accuracy: 0.1373\n",
      "Epoch 10/50\n",
      "170/170 [==============================] - 5s 27ms/step - loss: 5.3123 - accuracy: 0.1401\n",
      "Epoch 11/50\n",
      "170/170 [==============================] - 5s 28ms/step - loss: 5.2570 - accuracy: 0.1427\n",
      "Epoch 12/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 5.2064 - accuracy: 0.1444\n",
      "Epoch 13/50\n",
      "170/170 [==============================] - 5s 28ms/step - loss: 5.1640 - accuracy: 0.1475\n",
      "Epoch 14/50\n",
      "170/170 [==============================] - 4s 26ms/step - loss: 5.1185 - accuracy: 0.1521\n",
      "Epoch 15/50\n",
      "170/170 [==============================] - 5s 27ms/step - loss: 5.0714 - accuracy: 0.1553\n",
      "Epoch 16/50\n",
      "170/170 [==============================] - 5s 27ms/step - loss: 5.0159 - accuracy: 0.1624\n",
      "Epoch 17/50\n",
      "170/170 [==============================] - 4s 25ms/step - loss: 4.9646 - accuracy: 0.1660\n",
      "Epoch 18/50\n",
      "170/170 [==============================] - 4s 26ms/step - loss: 4.9150 - accuracy: 0.1692\n",
      "Epoch 19/50\n",
      "170/170 [==============================] - 4s 25ms/step - loss: 4.8687 - accuracy: 0.1733\n",
      "Epoch 20/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 4.8286 - accuracy: 0.1754\n",
      "Epoch 21/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 4.7872 - accuracy: 0.1795\n",
      "Epoch 22/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 4.7478 - accuracy: 0.1819\n",
      "Epoch 23/50\n",
      "170/170 [==============================] - 5s 27ms/step - loss: 4.7086 - accuracy: 0.1813\n",
      "Epoch 24/50\n",
      "170/170 [==============================] - 5s 30ms/step - loss: 4.6717 - accuracy: 0.1862\n",
      "Epoch 25/50\n",
      "170/170 [==============================] - 4s 24ms/step - loss: 4.6351 - accuracy: 0.1877\n",
      "Epoch 26/50\n",
      "170/170 [==============================] - 5s 29ms/step - loss: 4.5957 - accuracy: 0.1893\n",
      "Epoch 27/50\n",
      "170/170 [==============================] - 4s 26ms/step - loss: 4.5574 - accuracy: 0.1923\n",
      "Epoch 28/50\n",
      "170/170 [==============================] - 4s 26ms/step - loss: 4.5210 - accuracy: 0.1934\n",
      "Epoch 29/50\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 4.4874 - accuracy: 0.1965\n",
      "Epoch 30/50\n",
      "170/170 [==============================] - 4s 24ms/step - loss: 4.4512 - accuracy: 0.1975\n",
      "Epoch 31/50\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 4.4091 - accuracy: 0.2007\n",
      "Epoch 32/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 4.3712 - accuracy: 0.2022\n",
      "Epoch 33/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 4.3288 - accuracy: 0.2043\n",
      "Epoch 34/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 4.2916 - accuracy: 0.2065\n",
      "Epoch 35/50\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 4.2522 - accuracy: 0.2093\n",
      "Epoch 36/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 4.2078 - accuracy: 0.2108\n",
      "Epoch 37/50\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 4.1677 - accuracy: 0.2158\n",
      "Epoch 38/50\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 4.1276 - accuracy: 0.2158\n",
      "Epoch 39/50\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 4.0898 - accuracy: 0.2194\n",
      "Epoch 40/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 4.0508 - accuracy: 0.2218\n",
      "Epoch 41/50\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 4.0102 - accuracy: 0.2234\n",
      "Epoch 42/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 3.9733 - accuracy: 0.2266\n",
      "Epoch 43/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.9332 - accuracy: 0.2317\n",
      "Epoch 44/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.9006 - accuracy: 0.2337\n",
      "Epoch 45/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 3.8657 - accuracy: 0.2379\n",
      "Epoch 46/50\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 3.8312 - accuracy: 0.2410\n",
      "Epoch 47/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.7967 - accuracy: 0.2417\n",
      "Epoch 48/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.7670 - accuracy: 0.2453\n",
      "Epoch 49/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.7314 - accuracy: 0.2515\n",
      "Epoch 50/50\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 3.6950 - accuracy: 0.2526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7278f0f70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,50, input_length=seq_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size=128\n",
    "epochs=50\n",
    "\n",
    "model.fit(X,y, batch_size=batch_size,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nexword.h5')\n",
    "dump(tokenizer, open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_seq(model,tokenizer,seq_length, seed,length,n_words):\n",
    "    result=list()\n",
    "    in_text=seed_text\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        encoded=tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded= pad_sequences([encoded],maxlen=seq_length, truncating='pre')\n",
    "        predict_x=model.predict(encoded)\n",
    "        yhat=np.argmax(predict_x,axis=1)\n",
    "\n",
    "        out_word=''\n",
    "\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word =word\n",
    "                break\n",
    "        in_text += ' '+ out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
